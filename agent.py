import os
import sys
import logging
from pathlib import Path
from typing import List

from dotenv import load_dotenv
import tiktoken
from openai import OpenAI, AuthenticationError, OpenAIError

# ========== –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
MODEL = "gpt-4o"
MAX_TOKENS_PER_CHUNK = 3000          # ‚âà 6k —Å–ª–æ–≤
MAX_TOKENS_IN_COMPLETION = 1500      # –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –ø–æ–¥ –æ—Ç–≤–µ—Ç

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s: %(message)s",
)

# ========== –ó–∞–≥—Ä—É–∑–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

for var in ["HTTP_PROXY", "HTTPS_PROXY"]:
    value = os.getenv(var)
    if value:
        os.environ[var] = value
        logging.info(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è {var}")

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    logging.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω OPENAI_API_KEY")
    sys.exit(1)

client = OpenAI(api_key=api_key)

# ========== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def validate_token() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ API-–∫–ª—é—á OpenAI."""
    try:
        client.models.list()
        logging.info("‚úÖ API-–∫–ª—é—á —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω.")
        return True
    except AuthenticationError:
        logging.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π API-–∫–ª—é—á: OpenAI –æ—Ç–∫–∞–∑–∞–ª –≤ –¥–æ—Å—Ç—É–ø–µ.")
        return False
    except OpenAIError as e:
        logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ç–æ–∫–µ–Ω–∞: {e}")
        return False

def encoder_for(model: str):
    """
    –í tiktoken gpt-4o –ø–æ–∫–∞ –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å, –ø–æ—ç—Ç–æ–º—É –±–µ—Ä—ë–º –±–∞–∑–æ–≤—ã–π cl100k_base.
    """
    try:
        return tiktoken.encoding_for_model(model)
    except KeyError:
        return tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str, model: str) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
    enc = encoder_for(model)
    return len(enc.encode(text))

def chunk_text(text: str, model: str, max_tokens: int) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ —Ç–æ–∫–µ–Ω–∞–º, –∞ –Ω–µ –ø–æ —Å–ª–æ–≤–∞–º.
    """
    enc = encoder_for(model)
    tokens = enc.encode(text)
    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i : i + max_tokens]
        chunks.append(enc.decode(chunk_tokens))
    return chunks


# ========== OpenAI –≤—ã–∑–æ–≤—ã
def chat_completion(messages, model=MODEL, temperature=0.2):
    """
    –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—Ö–æ–¥ –≤ OpenAI. –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏.
    """
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=MAX_TOKENS_IN_COMPLETION,
        )
        return resp.choices[0].message.content.strip()
    except OpenAIError as e:
        logging.error(f"‚ùå OpenAI error: {e}")
        return ""


# ========== –ë–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ 
def summarise_file(fp: Path) -> str:
    """
    –î–µ–ª–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç —Ñ–∞–π–ª–∞, —á—Ç–æ–±—ã –ø–æ–∑–∂–µ
    –º–æ–∂–Ω–æ –±—ã–ª–æ —Å—Ç—Ä–æ–∏—Ç—å —Å–≤–æ–¥–Ω—ã–π –æ–±–∑–æ—Ä.
    """
    text = fp.read_text(encoding="utf-8")
    chunks = chunk_text(text, MODEL, MAX_TOKENS_PER_CHUNK)

    summaries = []
    for i, chunk in enumerate(chunks, 1):
        messages = [
            {"role": "system", "content": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-—Ä–µ–≤—å—é–µ—Ä –∫–æ–¥–∞. –°–¥–µ–ª–∞–π –æ—á–µ–Ω—å –∫—Ä–∞—Ç–∫–æ–µ (5-7 –ø—É–Ω–∫—Ç–æ–≤) —Ä–µ–∑—é–º–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–¥–∞ –Ω–∏–∂–µ. –ù–µ –ø–∏—à–∏ —É–ª—É—á—à–µ–Ω–∏—è, —Ç–æ–ª—å–∫–æ —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∫–æ–¥.  –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
            {"role": "user", "content": chunk},
        ]
        summary = chat_completion(messages)
        summaries.append(f"‚Äì –ß–∞—Å—Ç—å {i}: {summary}")
    return "\n".join(summaries)

def review_single_file(fp: Path):
    """
    –ê–Ω–∞–ª–∏–∑ –µ–¥–∏–Ω–∏—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    """
    if not fp.is_file():
        logging.warning(f"‚ö†Ô∏è –§–∞–π–ª {fp} –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return

    text = fp.read_text(encoding="utf-8")
    chunks = chunk_text(text, MODEL, MAX_TOKENS_PER_CHUNK)

    logging.info(f"=== –ö–æ–¥-—Ä–µ–≤—å—é —Ñ–∞–π–ª–∞ {fp.name} ===")
    for i, chunk in enumerate(chunks, 1):
        messages = [
            {"role": "system", "content": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-—Ä–µ–≤—å—é–µ—Ä –∫–æ–¥–∞. –ù–∞–π–¥–∏ –±–∞–≥–∏, –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã, –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è –∏ –∫—Ä–∞—Ç–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã, —Å–¥–µ–ª–∞–π –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –∫–æ–¥–∞. –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
            {"role": "user", "content": f"–ß–∞—Å—Ç—å {i} —Ñ–∞–π–ª–∞ {fp.name}:\n{chunk}"},
        ]
        answer = chat_completion(messages)
        logging.info(f"--- –†–µ–∑—É–ª—å—Ç–∞—Ç —á–∞—Å—Ç–∏ {i} ---\n{answer}\n")


def review_multiple_files(files: List[Path]):
    """
    –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤—Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏
    """
    # 1. –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ä–µ–∑—é–º–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    file_summaries = {}
    for fp in files:
        if not fp.is_file():
            logging.warning(f"‚ö†Ô∏è {fp} –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        logging.info(f"üìÑ –ö–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä—É–µ–º {fp.name} ...")
        file_summaries[fp.name] = summarise_file(fp)
    # 2. –°–æ–±–∏—Ä–∞–µ–º –µ–¥–∏–Ω—ã–π ¬´–∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞¬ª
    project_context = []
    for name, summary in file_summaries.items():
        project_context.append(f"### {name}\n{summary}")
    project_overview = "\n\n".join(project_context)
    # 3. –î–µ–ª–∏–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    chunks = chunk_text(project_overview, MODEL, MAX_TOKENS_PER_CHUNK)
    logging.info("=== –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞ ===")
    for i, chunk in enumerate(chunks, 1):
        messages = [
            {"role": "system", "content": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ü–û. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã —Å–æ–≤–º–µ—Å—Ç–Ω–æ, –Ω–∞–π–¥–∏ –ø—Ä–æ–±–ª–µ–º—ã –≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏, –ø—Ä–µ–¥–ª–æ–∂–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –¥–∞–π –∫—Ä–∞—Ç–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã, —Å–¥–µ–ª–∞–π –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É –∫–æ–¥–∞.. –§–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞: " + ", ".join(file_summaries.keys()) +". –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º."},
            {"role": "user", "content": f"–ß–∞—Å—Ç—å {i} —Å–≤–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞:\n{chunk}"},
        ]
        answer = chat_completion(messages)
        logging.info(f"--- –†–µ–∑—É–ª—å—Ç–∞—Ç —á–∞—Å—Ç–∏ {i} ---\n{answer}\n")


def review_code(paths: List[Path]):
    if len(paths) == 1:
        review_single_file(paths[0])
    else:
        review_multiple_files(paths)


# ========== –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    if len(sys.argv) < 2:
        logging.error("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python agent.py –ø—É—Ç—å/–∫/—Ñ–∞–π–ª—É1.py [—Ñ–∞–π–ª2.js ...]")
        sys.exit(1)

    if not validate_token():
        sys.exit(1)

    targets = [Path(p) for p in sys.argv[1:]]
    review_code(targets)